{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '.venv_shell_ai' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/Users/manuhalapeth/Downloads/Shell_ML_Challenge_2025/.venv_shell_ai/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# ==================================\n",
    "# Shell.ai 2025 - 08_modeling_prop1_prop7_boosted_STACKED_PERFORMANCE.py\n",
    "# ==================================\n",
    "\n",
    "import pandas as pd  # Data handling\n",
    "import numpy as np  # Numerical computations\n",
    "import joblib  # For saving/loading preprocessed data and models\n",
    "import os  # File path operations\n",
    "import warnings  # Suppress warnings\n",
    "from sklearn.model_selection import KFold  # Cross-validation\n",
    "from lightgbm import LGBMRegressor  # Gradient boosting base model\n",
    "from lightgbm.callback import early_stopping  # Early stopping in LGBM\n",
    "from sklearn.feature_selection import SelectFromModel  # Feature selection\n",
    "from sklearn.preprocessing import PowerTransformer  # Target transformation\n",
    "from sklearn.decomposition import PCA  # Dimensionality reduction\n",
    "from sklearn.cluster import KMeans  # Clustering for feature\n",
    "from sklearn.ensemble import StackingRegressor  # Stacked ensemble\n",
    "from sklearn.linear_model import RidgeCV  # Linear regression with CV for final estimator\n",
    "\n",
    "# Suppress warnings for cleaner notebook/log outputs\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# =============================\n",
    "#  Custom Shell MAPE Metric\n",
    "# =============================\n",
    "def shell_mape(y_true, y_pred, epsilon=1e-6):\n",
    "    \"\"\"\n",
    "    Compute Mean Absolute Percentage Error with epsilon to avoid division by zero.\n",
    "    Ignores near-zero true values for stability.\n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_true)  # Ensure numpy arrays\n",
    "    y_pred = np.asarray(y_pred)\n",
    "    non_zero_mask = np.abs(y_true) > epsilon  # Ignore very small true values\n",
    "    if not np.any(non_zero_mask):  # Edge case: all values effectively zero\n",
    "        return np.inf\n",
    "    mape = np.mean(\n",
    "        np.abs((y_true[non_zero_mask] - y_pred[non_zero_mask]) /\n",
    "               (np.abs(y_true[non_zero_mask]) + epsilon))\n",
    "    )\n",
    "    return mape\n",
    "\n",
    "# =============================\n",
    "#  Feature Engineering Functions\n",
    "# =============================\n",
    "def add_blend_weighted_features(df):\n",
    "    \"\"\"Compute weighted sum of component properties by blend fractions.\"\"\"\n",
    "    df_copy = df.copy()\n",
    "    for j in range(1, 11):  # For each target property\n",
    "        df_copy[f\"BlendWeighted_Property{j}\"] = 0.0  # Initialize column\n",
    "        for i in range(1, 6):  # For each component\n",
    "            blend_col = f\"Component{i}_fraction\"\n",
    "            prop_col = f\"Component{i}_Property{j}\"\n",
    "            if blend_col in df_copy.columns and prop_col in df_copy.columns:\n",
    "                df_copy[f\"BlendWeighted_Property{j}\"] += df_copy[blend_col] * df_copy[prop_col]\n",
    "    return df_copy\n",
    "\n",
    "def add_nonlinear_blend_transforms(df):\n",
    "    \"\"\"Apply nonlinear transformations (sq, cube, sqrt, log, inv) to blend fractions.\"\"\"\n",
    "    df_copy = df.copy()\n",
    "    for i in range(1, 6):\n",
    "        col = f\"Component{i}_fraction\"\n",
    "        if col in df_copy.columns:\n",
    "            df_copy[f\"{col}_sq\"] = df_copy[col] ** 2\n",
    "            df_copy[f\"{col}_sqrt\"] = np.sqrt(df_copy[col].clip(lower=0))  # Avoid negative sqrt\n",
    "            df_copy[f\"{col}_log\"] = np.log1p(df_copy[col].clip(lower=1e-6))  # Avoid log(0)\n",
    "            df_copy[f\"{col}_cube\"] = df_copy[col] ** 3\n",
    "            df_copy[f\"{col}_inv\"] = 1 / (df_copy[col] + 1e-6)  # Avoid div by zero\n",
    "    return df_copy\n",
    "\n",
    "def add_interaction_terms(df):\n",
    "    \"\"\"Add pairwise interaction terms between blend fractions.\"\"\"\n",
    "    df_copy = df.copy()\n",
    "    cols = [f\"Component{i}_fraction\" for i in range(1, 6)]\n",
    "    cols = [c for c in cols if c in df_copy.columns]  # Only existing columns\n",
    "    for i in range(len(cols)):\n",
    "        for j in range(i + 1, len(cols)):\n",
    "            df_copy[f\"{cols[i].replace('fraction', '')}_x{cols[j].replace('_fraction', '')}\"] = df_copy[cols[i]] * df_copy[cols[j]]\n",
    "    return df_copy\n",
    "\n",
    "def add_pca_features(df, n_components=3):\n",
    "    \"\"\"Reduce blend fraction dimensions using PCA and add as features.\"\"\"\n",
    "    df_copy = df.copy()\n",
    "    cols = [f\"Component{i}_fraction\" for i in range(1, 6)]\n",
    "    cols = [c for c in cols if c in df_copy.columns]\n",
    "    if not cols or df_copy[cols].empty:  # Skip if no relevant columns\n",
    "        return df_copy\n",
    "    temp_df = df_copy[cols].copy().fillna(0)\n",
    "    n_components = min(n_components, temp_df.shape[1])  # Adjust if fewer columns\n",
    "    if n_components > 0 and temp_df.shape[0] > 0:\n",
    "        pca = PCA(n_components=n_components, random_state=42)\n",
    "        pca_features = pca.fit_transform(temp_df)\n",
    "        for i in range(pca_features.shape[1]):\n",
    "            df_copy[f\"pca_{i}\"] = pca_features[:, i]\n",
    "    return df_copy\n",
    "\n",
    "def add_kmeans_cluster_feature(df, n_clusters=5):\n",
    "    \"\"\"Cluster blend fractions and add cluster label as a feature.\"\"\"\n",
    "    df_copy = df.copy()\n",
    "    comp_cols = [f\"Component{i}_fraction\" for i in range(1, 6)]\n",
    "    comp_cols = [c for c in comp_cols if c in df_copy.columns]\n",
    "    if not comp_cols or df_copy[comp_cols].empty:\n",
    "        df_copy[\"blend_cluster\"] = -1\n",
    "        return df_copy\n",
    "    temp_df = df_copy[comp_cols].copy().fillna(0)\n",
    "    n_clusters = min(n_clusters, temp_df.shape[0] // 5 + 1)  # Ensure at least ~5 samples per cluster\n",
    "    if n_clusters < 2:\n",
    "        n_clusters = 2\n",
    "    try:\n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "        df_copy[\"blend_cluster\"] = kmeans.fit_predict(temp_df)\n",
    "    except ValueError:  # Fallback for very small datasets\n",
    "        df_copy[\"blend_cluster\"] = -1\n",
    "    return df_copy\n",
    "\n",
    "# =============================\n",
    "#  Load Processed Data\n",
    "# =============================\n",
    "print(\"\\nðŸ“… Loading processed training and test data\")\n",
    "X_y = joblib.load(\"data/processed/train_processed.pkl\")  # Preprocessed train\n",
    "X_test = joblib.load(\"data/processed/test_processed.pkl\")  # Preprocessed test\n",
    "X_raw, y = X_y\n",
    "X_test_raw = X_test\n",
    "\n",
    "if isinstance(y, pd.Series):\n",
    "    y = y.to_frame()  # Ensure multi-output consistency\n",
    "\n",
    "print(f\"Train X (raw): {X_raw.shape}\")\n",
    "print(f\"Train y: {y.shape}\")\n",
    "print(f\"Test X (raw): {X_test_raw.shape}\")\n",
    "\n",
    "# Copy for processing\n",
    "X_processed = X_raw.copy()\n",
    "X_test_processed = X_test_raw.copy()\n",
    "\n",
    "# =============================\n",
    "#  Apply Feature Engineering\n",
    "# =============================\n",
    "print(\"Applying feature engineering...\")\n",
    "for func in [add_blend_weighted_features, add_nonlinear_blend_transforms,\n",
    "             add_interaction_terms, add_pca_features, add_kmeans_cluster_feature]:\n",
    "    X_processed = func(X_processed)\n",
    "    X_test_processed = func(X_test_processed)\n",
    "\n",
    "# Fill missing values post-feature engineering\n",
    "X_processed.fillna(0, inplace=True)\n",
    "X_test_processed.fillna(0, inplace=True)\n",
    "\n",
    "# =============================\n",
    "#  Align Columns Between Train/Test\n",
    "# =============================\n",
    "train_cols = set(X_processed.columns)\n",
    "test_cols = set(X_test_processed.columns)\n",
    "common_cols = list(train_cols.intersection(test_cols))\n",
    "\n",
    "# Add missing columns in train/test to maintain alignment\n",
    "for col in (train_cols - test_cols):\n",
    "    X_test_processed[col] = 0.0\n",
    "for col in (test_cols - train_cols):\n",
    "    X_processed[col] = 0.0\n",
    "\n",
    "# Ensure consistent column order\n",
    "X_processed = X_processed[common_cols].reindex(columns=sorted(common_cols))\n",
    "X_test_processed = X_test_processed[common_cols].reindex(columns=sorted(common_cols))\n",
    "\n",
    "# Convert object columns to numeric\n",
    "for col in X_processed.columns:\n",
    "    X_processed[col] = pd.to_numeric(X_processed[col], errors='coerce')\n",
    "    X_test_processed[col] = pd.to_numeric(X_test_processed[col], errors='coerce')\n",
    "\n",
    "# Fill remaining missing values\n",
    "X_processed.fillna(0, inplace=True)\n",
    "X_test_processed.fillna(0, inplace=True)\n",
    "\n",
    "X_processed_np = X_processed.values  # Convert to NumPy for model\n",
    "X_test_processed_np = X_test_processed.values\n",
    "\n",
    "# =============================\n",
    "#  Cross-validation and Stacking Setup\n",
    "# =============================\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)  # 5-fold CV\n",
    "seeds = [42, 2024]  # Multiple seeds for model ensembling\n",
    "\n",
    "cv_total = []  # Store per-target CV scores\n",
    "target_cols = y.columns.tolist()\n",
    "test_preds_all = np.zeros((X_test_processed_np.shape[0], len(target_cols)))  # Store final test preds\n",
    "\n",
    "special_targets = [\"BlendProperty1\", \"BlendProperty4\", \"BlendProperty5\",\n",
    "                   \"BlendProperty7\", \"BlendProperty9\"]  # Could be used for extra tuning\n",
    "es_callback = early_stopping(stopping_rounds=70, verbose=False)  # Early stopping LGBM\n",
    "\n",
    "# =============================\n",
    "#  Modeling Loop per Target\n",
    "# =============================\n",
    "for t_idx, target in enumerate(target_cols):\n",
    "    print(f\"\\n Target: {target}\")\n",
    "\n",
    "    pt = PowerTransformer(method='yeo-johnson')  # Stabilize target distribution\n",
    "    y_trans = pt.fit_transform(y[[target]])[:, 0]  # Transform target\n",
    "\n",
    "    # Feature selection using LightGBM\n",
    "    selector_model = LGBMRegressor(n_estimators=150, random_state=0, n_jobs=-1)\n",
    "    selector_model.fit(X_processed_np, y[target])\n",
    "    max_feats = 120 if target == \"BlendProperty1\" else (100 if target == \"BlendProperty7\" else 60)\n",
    "    selector = SelectFromModel(selector_model, prefit=True, max_features=max_feats, threshold=-np.inf)\n",
    "    X_sel = selector.transform(X_processed_np)\n",
    "    X_test_sel = selector.transform(X_test_processed_np)\n",
    "    print(f\"Selected initial features shape: {X_sel.shape}\")\n",
    "\n",
    "    oof_preds = np.zeros(X_processed_np.shape[0])  # Out-of-fold predictions\n",
    "    test_preds = np.zeros(X_test_processed_np.shape[0])\n",
    "\n",
    "    # Seed ensembling\n",
    "    for seed in seeds:\n",
    "        fold_preds = np.zeros(X_processed_np.shape[0])\n",
    "        fold_test = np.zeros(X_test_processed_np.shape[0])\n",
    "\n",
    "        for fold, (tr_idx, val_idx) in enumerate(kf.split(X_sel)):\n",
    "            X_tr, X_val = X_sel[tr_idx], X_sel[val_idx]\n",
    "            y_tr, y_val = y_trans[tr_idx], y_trans[val_idx]\n",
    "\n",
    "            # Base models for stacking\n",
    "            lgbm_base_model = LGBMRegressor(\n",
    "                n_estimators=500,\n",
    "                learning_rate=0.02,\n",
    "                max_depth=5,\n",
    "                num_leaves=32,\n",
    "                min_child_samples=20,\n",
    "                subsample=0.7,\n",
    "                reg_alpha=0.1,\n",
    "                reg_lambda=0.1,\n",
    "                random_state=seed,\n",
    "                n_jobs=-1\n",
    "            )\n",
    "\n",
    "            estimators = [\n",
    "                ('lgbm', lgbm_base_model),\n",
    "                ('ridge', RidgeCV(alphas=np.logspace(-3, 3, 7), cv=kf))  # Meta-features from Ridge\n",
    "            ]\n",
    "\n",
    "            # Stacking ensemble\n",
    "            stack_model = StackingRegressor(\n",
    "                estimators=estimators,\n",
    "                final_estimator=RidgeCV(alphas=np.logspace(-3, 3, 3)),  # Final linear blend\n",
    "                cv=kf,\n",
    "                n_jobs=-1\n",
    "            )\n",
    "\n",
    "            # Fit stacking model\n",
    "            stack_model.fit(X_tr, y_tr)\n",
    "\n",
    "            # Store fold predictions\n",
    "            fold_preds[val_idx] = stack_model.predict(X_val)\n",
    "            fold_test += stack_model.predict(X_test_sel) / kf.get_n_splits()\n",
    "\n",
    "        # Average across seeds\n",
    "        oof_preds += fold_preds / len(seeds)\n",
    "        test_preds += fold_test / len(seeds)\n",
    "\n",
    "    # Inverse transform predictions to original scale\n",
    "    oof_preds_inv = pt.inverse_transform(oof_preds.reshape(-1, 1))[:, 0]\n",
    "    test_preds_inv = pt.inverse_transform(test_preds.reshape(-1, 1))[:, 0]\n",
    "\n",
    "    # Clip extreme predictions to avoid unrealistic values\n",
    "    q_low, q_high = y[target].quantile([0.005, 0.995])\n",
    "    test_preds_inv = np.clip(test_preds_inv, q_low, q_high)\n",
    "    oof_preds_inv = np.clip(oof_preds_inv, q_low, q_high)\n",
    "\n",
    "    cv_score = shell_mape(y[target], oof_preds_inv)  # CV metric\n",
    "    print(f\" CV MAPE: {cv_score:.6f}\")\n",
    "\n",
    "    test_preds_all[:, t_idx] = test_preds_inv  # Store test predictions\n",
    "    cv_total.append(cv_score)  # Store CV score\n",
    "\n",
    "# =============================\n",
    "#  CV Summary\n",
    "# =============================\n",
    "print(\"\\n====================\")\n",
    "print(\" CV MAPE per target:\")\n",
    "for col, score in zip(target_cols, cv_total):\n",
    "    print(f\"{col}: {score:.6f}\")\n",
    "print(f\"\\n Avg CV MAPE: {np.mean(cv_total):.6f}\")\n",
    "\n",
    "# =============================\n",
    "#  Save Submission\n",
    "# =============================\n",
    "os.makedirs(\"submissions\", exist_ok=True)  # Ensure folder exists\n",
    "script_dir = os.path.dirname(__file__) if '__file__' in locals() else os.getcwd()\n",
    "test_csv_paths = [\n",
    "    os.path.join(script_dir, \"data\", \"raw\", \"test.csv\"),\n",
    "    os.path.join(script_dir, \"..\", \"data\", \"raw\", \"test.csv\"),\n",
    "    os.path.join(script_dir, \"..\", \"..\", \"data\", \"raw\", \"test.csv\"),\n",
    "    \"data/raw/test.csv\",\n",
    "]\n",
    "\n",
    "# Load raw test CSV\n",
    "test_raw = None\n",
    "for path in test_csv_paths:\n",
    "    if os.path.exists(path):\n",
    "        test_raw = pd.read_csv(path)\n",
    "        break\n",
    "if test_raw is None:\n",
    "    raise FileNotFoundError(\"test.csv not found in expected locations.\")\n",
    "\n",
    "id_col = \"ID\" if \"ID\" in test_raw.columns else test_raw.columns[0]\n",
    "\n",
    "# Prepare submission\n",
    "submission = pd.DataFrame({id_col: test_raw[id_col]})\n",
    "for i, col in enumerate(target_cols):\n",
    "    submission[col] = test_preds_all[:, i]\n",
    "\n",
    "submission.to_csv(\"submissions/prop1_prop7_boosted_STACKED_PERFORMANCE_submission.csv\", index=False)\n",
    "print(\"\\n Submission saved to submissions/prop1_prop7_boosted_STACKED_PERFORMANCE_submission.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
